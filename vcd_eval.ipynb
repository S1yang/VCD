{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# VCD-based Open-Ended Caption Generation Script\n",
    "# This notebook loads a VCD-enabled LVLM, processes a list of image IDs,\n",
    "# and generates open-ended captions using the optimal settings reported in the VCD paper.\n",
    "\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# %%\n",
    "# Add paths to the VCD repository and submodules\n",
    "import sys\n",
    "repo_root = os.path.abspath(os.getcwd())\n",
    "\n",
    "sys.path.insert(0, os.path.join(repo_root, \"experiments\"))\n",
    "sys.path.insert(0, os.path.join(repo_root, \"vcd_utils\"))\n",
    "\n",
    "# %%\n",
    "# Import LVLM and VCD utilities\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\n",
    "from vcd_utils.vcd_add_noise import add_diffusion_noise\n",
    "from vcd_utils.vcd_sample import evolve_vcd_sampling\n",
    "\n",
    "evolve_vcd_sampling()  # initialize VCD sampling functions\n",
    "\n",
    "# %%\n",
    "# Configuration\n",
    "\n",
    "txt_file = \"../../auto_cir/selected_images-160.txt\"\n",
    "img_dir = \"../../img-set/val2014\"\n",
    "output_jsonl = \"gen_captions.jsonl\"\n",
    "model_path = \"/root/.cache/huggingface/hub/models--liuhaotian--llava-v1.5-7b/snapshots/4481d270cc22fd5c4d1bb5df129622006ccd9234\"     \n",
    "model_base = None                          # if using LoRA weights, set base model here\n",
    "conv_mode = \"llava_v1\"                   # conversational template for captioning\n",
    "use_cd = True                              # enable VCD\n",
    "\n",
    "# VCD hyperparameters (as per Appendix A)\n",
    "noise_step = 500   # T for LLaVA-Bench-style open-ended generation\n",
    "cd_alpha = 1.0    # α\n",
    "cd_beta = 0.1     # β\n",
    "\n",
    "# Sampling parameters\n",
    "temperature = 1.0\n",
    "top_p = 1.0\n",
    "top_k = None\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Load the pretrained LVLM with VCD capabilities\n",
    "disable_torch_init()\n",
    "\n",
    "model_name = \"llava-1.5-7b\"\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path,\n",
    "    model_base,\n",
    "    model_name,\n",
    "    load_8bit=False,\n",
    "    load_4bit=False,\n",
    "    device=device\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# %%\n",
    "# Read image IDs from the text file\n",
    "with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    image_files = [line.strip() for line in f if line.strip()]\n",
    "print(f\"Total images: {len(image_files)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.constants import DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "\n",
    "with open(output_jsonl, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for image_file in tqdm(image_files, desc=\"Generating captions\"):\n",
    "        # 从文件名提取 ID\n",
    "        stem = os.path.splitext(image_file)[0]     \n",
    "        image_id = int(stem.split(\"_\")[-1])        \n",
    "\n",
    "        # 加载图像\n",
    "        path = os.path.join(img_dir, image_file)\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        # 构造对话 prompt（只用 image token）\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            img_token = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + \"\\n\"\n",
    "        else:\n",
    "            img_token = DEFAULT_IMAGE_TOKEN + \"\\n\"\n",
    "        conv = conv_templates[conv_mode].copy()\n",
    "        conv.append_message(conv.roles[0], img_token)\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "        prompt = conv.get_prompt()\n",
    "\n",
    "        # Tokenize + 图像预处理\n",
    "        input_ids = tokenizer_image_token(\n",
    "            prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\"\n",
    "        ).unsqueeze(0).to(device)\n",
    "        pixel_values = image_processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"][0]\n",
    "\n",
    "        # VCD 噪声图\n",
    "        noisy = add_diffusion_noise(pixel_values, noise_step) if use_cd else None\n",
    "\n",
    "        # 推理\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                images=pixel_values.unsqueeze(0).half().to(device),\n",
    "                images_cd=noisy.unsqueeze(0).half().to(device) if noisy is not None else None,\n",
    "                cd_alpha=cd_alpha,\n",
    "                cd_beta=cd_beta,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                top_k=top_k,\n",
    "                max_new_tokens=128,\n",
    "            )\n",
    "\n",
    "        # 解码并清理尾部 stop_str（如果还需要）\n",
    "        gen_ids = outputs[0, input_ids.shape[1]:].cpu().tolist()\n",
    "        gen = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        # # DEBUG 打印\n",
    "        # print(\"PROMPT:\", prompt)\n",
    "        # print(\"INPUT IDS:\", input_ids[0].cpu().tolist())\n",
    "        # print(\"GENERATED IDS:\", gen_ids)\n",
    "        # print(\"DECODED:\", gen)\n",
    "\n",
    "        # 写入 JSONL\n",
    "        out_file.write(\n",
    "            json.dumps({\"image_id\": image_id, \"caption\": gen}, ensure_ascii=False)\n",
    "            + \"\\n\"\n",
    "        )\n",
    "\n",
    "print(\"Done! Saved to\", output_jsonl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
